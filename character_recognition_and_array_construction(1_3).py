# -*- coding: utf-8 -*-
"""character_recognition_and_array_construction(1_3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dUNhdLtEaKkxidcdx8Ys29E4WZc1SeTN
"""

!pip install emnist

import os
import sys
sys.path.insert(1, os.path.abspath('../home/stefggl/.local/lib/python3.8/site-packages'))
print(sys.path)

#Load up EMNIST database and local one
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
from emnist import list_datasets
list_datasets()

from emnist import extract_training_samples
emnist_images, emnist_labels = extract_training_samples('letters')

bgty=[2,7,20,25]
bgty_indices=[]
lookup = { '2':1, '7':3, '20':0, '25':2 }

for i in range(len(emnist_labels)):
  if emnist_labels[i] in bgty:
    bgty_indices.append(i)

bgty_images_i=[]
bgty_labels_i=[]

for p in range(len(bgty_indices)):
  bgty_images_i.append(emnist_images[bgty_indices[p]])
  bgty_labels_i.append(lookup[str(emnist_labels[bgty_indices[p]])])

print(str(len(bgty_images_i))+'\n'+str(len(bgty_labels_i)))

#Unzip the local folder
!unzip a_set_of_fonts.zip
!mkdir digital_data && mv *.png digital_data

#lists all filenames onto a file
!cd digital_data/ && ls > all_fonts.txt && ls | grep "all_fonts" | cat all_fonts.txt | less

import cv2 as cv
from google.colab.patches import cv2_imshow

#Read the lists of names to open corresponding images and save corresponding labels
#onto the corresponding array

path = sys.path[0]+"/digital_data/" #[0-22]

names = { 'T':0, 't':0, 'B':1, 'b':1, 'G':3, 'g':3, 'Y':2, 'y':2 }

local_images=[]
local_labels=[]

with open(path+"all_fonts.txt") as file:
  for line in file.readlines():
    filename = line.rstrip()
    if (filename != 'all_fonts.txt'):
      imaget= cv.imread(path+filename, flags = cv.IMREAD_GRAYSCALE)
      imaget = cv.bitwise_not(imaget)
      local_images.append(imaget)
      local_labels.append(names[filename[:1]])

print(str(len(local_images))+'\n'+str(len(local_labels)))

#Append the two arrays together to get the final dataset
images = np.concatenate((np.array(bgty_images_i), np.array(local_images)))

labels= np.array(bgty_labels_i + local_labels)

print(images.shape)
len(labels)

#Normalizing values
images= images.astype("float32")/255

"""# Now procedures are perfectly transparent to the database (we must remember to shuffle the data though)"""

X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.3, random_state=100)
print("Training images")
print(type(X_train))
print(X_train.shape)
print("Training labels")
print(type(y_train))
print(y_train.shape)
print("Testing images")
print(type(X_val))
print(X_val.shape)
print("Testing labels")
print(type(y_val))
print(y_val.shape)

from random import randint
import matplotlib.pyplot as plt

t= randint(0,19781)

plt.figure(figsize=(10,10))
for i,image in enumerate(X_train[t:t+25]):
    #image = imge.numpy().reshape((28,28))
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(image)
    #plt.xlabel(class_names[label])
plt.show()

"""#Model creation"""

f_v_l= 784
X_train = X_train.reshape(X_train.shape[0], f_v_l)
X_val = X_val.reshape(X_val.shape[0], f_v_l)
print(X_train.shape)
print(X_val.shape)

bgty_model = keras.Sequential()

bgty_model.add(layers.Dropout(
                        0.5,
                        input_shape = (f_v_l,)
))
bgty_model.add(layers.Dense(
                        1024,
                        kernel_initializer = "lecun_normal",
                        activation = "selu"
))
bgty_model.add(layers.Dense(
                        2048,
                        activation = "relu"
))
bgty_model.add(layers.Dense(
                        1024,
                        activation = "relu"
))
bgty_model.add(layers.Dense(
                        4,
                        activation = "softmax"
))

bgty_model.summary()

#Training and Evaluation

batch_size = 256
epochs = 10

bgty_model.compile(loss="sparse_categorical_crossentropy",
                  optimizer = "adam",
                  metrics = ["accuracy"])
history = bgty_model.fit(X_train, y_train, batch_size = 256, epochs=epochs,
                         validation_data=(X_val, y_val))

score= bgty_model.evaluate(X_val, y_val, verbose=0)

print(f"Test loss: {score[0]}\nTest Accuracy: {score[1]}")

!mkdir fonts/
!mv *Font*png fonts/

!cd output && touch file.jpg finalimg.jpg

#Single test
import matplotlib.pyplot as plt
import random as rand

one_case=rand.randrange(0,len(X_val))

predictions_single = bgty_model.predict(X_val[one_case:(one_case + 1)])
print(predictions_single)
print(np.sum(predictions_single))
#print(X_test[one_case:(one_case + 1)])



y_classes = predictions_single.argmax()
print("la predizione è: {}".format(y_classes))

x_test_vis = X_val[one_case:(one_case+1)].reshape(28, 28)
print(x_test_vis.shape)
plt.imshow(x_test_vis, cmap = plt.cm.binary)

import matplotlib.pyplot as plt
# Plot training & validation accuracy values
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

#max_prob=0.05
map_prob=1.00

def close_enough_linear(prob_array, labels):
  prob_array=np.asarray(prob_array)

  #= (X, 1, 4)
  if (prob_array.ndim == 3 and prob_array[0].shape == (1, 4)):
    #
    index,tmp = -1,0
    for i in range(len(labels)):
      #if (labels[i] == 0 or labels[i] == 3): #T or Y
      if (prob_array[i][0][0] > tmp):
        tmp = prob_array[i][0][0] #T
        index = i
    if (index != -1):
      print(f"Likeliest candidate is at index {index} with probability {tmp}")
      return index
    else:
      raise Exception("No Likely T was found in the data")
  else:
    raise Exception("Array of not (X, 1, 4) shape")
  #O(n) preciso

def close_enough_to_T(prob_array):
    #2-d array of probabilities
    prob_array= np.asarray(prob_array)
    #prob_array.shape = (X, 1, 4)
    if (prob_array.ndim == 3):
      i=1
      epsilon = sys.float_info.epsilon
      while (epsilon < max_prob):
          epsilon *= i
          for a in range(0,len(prob_array)):
              #i-esima cella: [p, 1-p] con p= prob. che sia T
              #i-esime cella: [0]prob. of B[.]...[2]prob. of T...
              if (prob_array[a][0][2] < 1.0 and prob_array[a][0][2] > (1.0-epsilon)):
                  return a #ho trovato l'indice di massima prob. che sia una T
          i+=10 
          #per come è definita la precisione decimale in python aumentare di 10 volte epsilon
          #"garantisce" un salto "abbastanza piccolo" nel range desiderato
      if (epsilon >= max_prob):
          raise Exception("there are no cells likely to be a T")
    else :
      raise Exception("Non 2-Dimensional array")

    #PROBLEMA: O(kN) con k=floor((10-array.max_value)/epsilon)!!!
        
#Trovata la più probabile T, tutte le altre T meno probabili diventano Y
#(Condizione 2, 5 ed eventualmente 6)
        
def ttt_out(array, trueish_t):
    #set the most likely image to be a t to the actual t's. All else to y
    array= np.asarray(array) #check for array to be of numpy.ndarray
    if (array.ndim == 1):
      for a in range(0,len(array)):
        if (a==trueish_t):
          array[a] = 0 #the most likely T
        else: 
          if (array[a] == 0): #if all other non-likely Ts
            array[a]=3 #Y
      return array
    else:
        raise Exception("Non 1-Dimensional array")

#Funzioni inizialmente scritte per scikit-learn, che supportavano la funzione predict_proba,
#che riportava array di probabilità, la quale non è presente su keras. ne creo una manuale

def get_prob_array(model, data):
  prob_array=[]
  prov_array=[]
  for one_case in range(0,len(data)):
    predictions_single = model.predict(data[one_case:(one_case + 1)])
    prob_array.append(predictions_single)
    prov_array.append(predictions_single.argmax())
  return prob_array, prov_array
        
#Impacchetto le due funzioni in un'unica chiamata
#Questa funzione prenderà in input il solo array delle Y e T
    
def cleandata(data, probabilities, labels):
    return ttt_out(data, close_enough_linear(probabilities, labels))

import cv2 as cv
from google.colab.patches import cv2_imshow

#carico le celle della griglia sul modello

dir=sys.path[0]
path="/output/"
i,width,height=0,0,0
data=[]




!cd output/ && ls > path1

#import image in dir
with open((dir + path + "path1")) as file:
  for line in file.readlines():
    keyword=line.rstrip()
    if (keyword != "path1" and
        keyword != "file.jpg" and
        keyword != "finalimg.jpg" and
        keyword != "photo.jpg"):
      i+=1
      filename=dir+path+keyword
      if(filename[(len(filename)-6):(len(filename)-4)] == "10"):
        #first row
        width = i-1
      print(filename)
      imaget = cv.imread(filename, flags= cv.IMREAD_GRAYSCALE)
      imaget = cv.bitwise_not(imaget)
      imaget= imaget.reshape(28,28)
      cv2_imshow(imaget)
        
      #Normalizzo le immagini
      #shape image as single array
      imaget=imaget.reshape(1, 784)
      imaget= imaget.astype("float32")/255
      data.append(imaget)

print(filename[(len(filename)- 6):(len(filename)- 4)])

height = i//width

print(f"width: {width}\nheight: {height:0.0f}")

probabilities, labels = get_prob_array(bgty_model, data)

for k in range(len(labels)):
  print(f"{probabilities[k]}")
print(np.asarray(probabilities).shape)
print(np.asarray(probabilities).ndim)
print(labels)
#passo le previsioni, dati e modello per ricavare il mio output
#finale

cleanup = cleandata(labels, probabilities, labels)

print(f"l'array finale è {cleanup}")

cleanup = [cleanup[a] for a in range(0,len(cleanup))]
cleanup += [cleanup.index(0)]
cleanup

(cleanup, width, height)
